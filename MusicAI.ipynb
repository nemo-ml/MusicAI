{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MusicAI.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HakimTem/MusicAI/blob/master/MusicAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyszefyG96TK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pull stuff from repository\n",
        "! git clone https://github.com/HakimTem/MusicAI.git\n",
        "% cd MusicAI\n",
        "! git pull\n",
        "\n",
        "# import statements for everything\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution()\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time\n",
        "import functools\n",
        "import base64\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Make sure everything is installed properly\n",
        "is_correct_tf_version = '1.13.1' in tf.__version__\n",
        "assert is_correct_tf_version, \"Wrong tensorflow version ({}) installed\".format(tf.__version__)\n",
        "\n",
        "is_eager_enabled = tf.executing_eagerly()\n",
        "assert is_eager_enabled,      \"Tensorflow eager mode is not enabled\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JyTFRbzVdopn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PeriodicPlotter:\n",
        "  def __init__(self, sec, xlabel='', ylabel='', scale=None):\n",
        "    from IPython import display as ipythondisplay\n",
        "    import matplotlib.pyplot as plt\n",
        "    import time\n",
        "\n",
        "    self.xlabel = xlabel\n",
        "    self.ylabel = ylabel\n",
        "    self.sec = sec\n",
        "    self.scale = scale\n",
        "\n",
        "    self.tic = time.time()\n",
        "\n",
        "  def plot(self, data):\n",
        "    if time.time() - self.tic > self.sec:\n",
        "      plt.cla()\n",
        "      \n",
        "      if self.scale is None:\n",
        "        plt.plot(data)\n",
        "      elif self.scale == 'semilogx':\n",
        "        plt.semilogx(data)\n",
        "      elif self.scale == 'semilogy':\n",
        "        plt.semilogy(data)\n",
        "      elif self.scale == 'loglog':\n",
        "        plt.loglog(data)\n",
        "      else:\n",
        "        raise ValueError(\"unrecognized parameter scale {}\".format(self.scale))\n",
        "\n",
        "      plt.xlabel(self.xlabel); plt.ylabel(self.ylabel)\n",
        "      ipythondisplay.clear_output(wait=True)\n",
        "      ipythondisplay.display(plt.gcf())\n",
        "\n",
        "      self.tic = time.time()\n",
        "      \n",
        "def custom_progress_text(message):\n",
        "  import progressbar\n",
        "  from string import Formatter\n",
        "\n",
        "  message_ = message.replace('(', '{')\n",
        "  message_ = message_.replace(')', '}')\n",
        "\n",
        "  keys = [key[1] for key in Formatter().parse(message_)]\n",
        "\n",
        "  ids = {}\n",
        "  for key in keys:\n",
        "    if key is not None:\n",
        "      ids[key] = float('nan')\n",
        "\n",
        "  msg = progressbar.FormatCustomText(message, ids)\n",
        "  return msg\n",
        "\n",
        "def create_progress_bar(text=None):\n",
        "  import progressbar\n",
        "  if text is None:\n",
        "    text = progressbar.FormatCustomText('')\n",
        "  bar = progressbar.ProgressBar(widgets=[\n",
        "      progressbar.Percentage(),\n",
        "      progressbar.Bar(),\n",
        "      progressbar.AdaptiveETA(), '  ',\n",
        "      text,\n",
        "  ])\n",
        "  return bar"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShPhKbR528N0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def MusicTotxt(sound_file):\n",
        "  \n",
        "  # use mode = \"rb\" to read binary file\n",
        "  fin = open(sound_file, \"rb\")\n",
        "  binary_data = fin.read()\n",
        "  fin.close()\n",
        "\n",
        "  # encode binary to base64 string (printable)\n",
        "  b64_data = base64.b64encode(binary_data)\n",
        "  b64_fname = \"Sneaky Snitch_b64.txt\"\n",
        "\n",
        "  # save base64 string to given text file\n",
        "  fout = open(b64_fname, \"w\")\n",
        "  fout.write(b64_data)\n",
        "  fout.close\n",
        "\n",
        "def txtToMP3(b64_file_name, mp3_file_name): \n",
        "  # Convert b64 to mp3\n",
        "  fin = open(b64_file_name, \"r\")\n",
        "  b64_str = fin.read()\n",
        "  fin.close()\n",
        "  mp3_data = base64.b64decode(b64_str)\n",
        "  \n",
        "  # Write to mp3 file\n",
        "  fout = open(mp3_file_name, \"w\")\n",
        "  fout.write(mp3_data)\n",
        "  fout.close\n",
        "  \n",
        "  return mp3_file_name\n",
        "\n",
        "def readEncodedText(text_file):\n",
        "  return Audio(filename=txtToMP3(text_file, \"Output.mp3\"))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gh2vs3lxW2i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make a copy of the text file in tensorflow\n",
        "filePath = tf.keras.utils.get_file('Sneaky_Snitch.txt', MusicTotxt('Sneaky Snitch.mp3'))\n",
        "\n",
        "# See how long the file is\n",
        "text = open(filePath).read()\n",
        "print(\"Length of Text: {} characters\".format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZg66Hi_8wOi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "readEncodedText(filePath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yQX88BCAFmQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2ebbc0e7-b962-4ffb-8161-6d2339b35f84"
      },
      "source": [
        "vocab = sorted(set(text))\n",
        "print(\"The file has {} unique characters\".format(len(vocab)))"
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The file has 64 unique characters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOvBc6lnBMhL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "'''TODO: Create a mapping from indices to characters'''\n",
        "idx2char = np.array(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Wo7tK-jB0Jt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('{')\n",
        "for char,_ in zip(char2idx, range(65)):\n",
        "    print('  {}: {},'.format(repr(char), char2idx[char]))\n",
        "print('  ...\\n}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74mY3JKuCXYx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"{} -----------> {}\".format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aveSTvkDoo_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequence_length = 3\n",
        "\n",
        "example_per_time = len(text) // sequence_length\n",
        "\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "sequences = char_dataset.batch(sequence_length + 1, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5LduWD2Fabr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(sequence): \n",
        "  inputText = sequence[:-1]\n",
        "  targetText = sequence[1:]\n",
        "  \n",
        "  return inputText, targetText\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EimSRf2PdYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch size \n",
        "BATCH_SIZE = 60\n",
        "\n",
        "steps_per_epoch = example_per_time//BATCH_SIZE\n",
        "\n",
        "# Buffer size is similar to a queue size\n",
        "# This defines a manageable data size to put into memory, where elements are shuffled\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xp1fbFXDUGIZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Make the basis for the LSTM (Long Short Term Memory) Unit\n",
        "LSTM = tf.keras.layers.CuDNNLSTM\n",
        "LSTM = functools.partial(LSTM, return_sequences=True, recurrent_initializer='glorot_uniform', stateful=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5oVs8QCnVQ6W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_network(output_size, embedding_dim, rnn_units, batch_size):\n",
        "  model = tf.keras.Sequential([tf.keras.layers.Embedding(output_size, embedding_dim, batch_input_shape = [batch_size, None]), LSTM(rnn_units), tf.keras.layers.Dense(output_size)])\n",
        "  return model\n",
        "\n",
        "print(len(vocab))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6lEICp4bWweN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "neural_network = build_network(len(vocab), 64, 1024, 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rL0YdfP6YnEg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1): \n",
        "  example_batch_predictions = neural_network(input_example_batch)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0bgjFaUZsrh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.multinomial(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zP-NOF0ybjP9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_error(target, logits): \n",
        "  tf.keras.backend.sparse_categorical_crossentropy(target, logits, from_logits = True)\n",
        "  \n",
        "example_batch_loss = calculate_error(target_example_batch, example_batch_predictions)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0-OQaaScywK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        },
        "outputId": "6c730f6f-0aee-47f6-e72d-31524001e1c7"
      },
      "source": [
        "\n",
        "# Training step\n",
        "EPOCHS = 5 \n",
        "'''TODO: experiment with different optimizers'''\n",
        "'''How does changing this affect the network's performance?'''\n",
        "optimizer = tf.train.AdamOptimizer() # TODO\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "history = []\n",
        "plotter = PeriodicPlotter(sec=1, xlabel='Iterations', ylabel='Loss')\n",
        "for epoch in range(EPOCHS):\n",
        "\n",
        "    # Initialize the hidden state at the start of every epoch; initially is None\n",
        "    hidden = neural_network.reset_states()\n",
        "    \n",
        "    # Enumerate the dataset for use in training\n",
        "    custom_msg = custom_progress_text(\"Loss: %(loss)2.2f\")\n",
        "    bar = create_progress_bar(custom_msg)\n",
        "    for inp, target in bar(dataset):\n",
        "        # Use tf.GradientTape()\n",
        "        with tf.GradientTape() as tape:\n",
        "            '''TODO: feed the current input into the model and generate predictions'''\n",
        "            predictions = neural_network(inp) # TODO\n",
        "            '''TODO: compute the loss!'''\n",
        "            loss = calculate_error(target, predictions) # TODO\n",
        "        \n",
        "        # Now, compute the gradients and try to minimize\n",
        "        '''TODO: complete the function call for gradient computation'''\n",
        "        grads = tape.gradient(loss, neural_network.trainable_variables) # TODO\n",
        "        optimizer.apply_gradients(zip(grads, neural_network.trainable_variables))\n",
        "        \n",
        "        # Update the progress bar!\n",
        "        history.append(loss.numpy().mean())\n",
        "        custom_msg.update_mapping(loss=history[-1])\n",
        "        plotter.plot(history)\n",
        "    \n",
        "    # Update the model with the changed weights!\n",
        "    neural_network.save_weights(checkpoint_prefix.format(epoch=epoch))\n"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r                                                                               \r\rN/A%|#                                               |ETA:  --:--:--  Loss: nan"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0mTraceback (most recent call last)",
            "\u001b[0;32m<ipython-input-155-85b44341fc74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# Now, compute the gradients and try to minimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;34m'''TODO: complete the function call for gradient computation'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# TODO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneural_network\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/backprop.pyc\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0mflat_sources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m    947\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/eager/imperative_grad.pyc\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m     70\u001b[0m       \u001b[0msources\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_id'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4duwVEQN8cuX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Push the encoded binary file to the repository\n",
        "! git config --global user.email \"hemacini@icloud.com\"\n",
        "! git config --global user.name \"MonaLisa\"\n",
        "! git add 'Sneaky Snitch_b64.txt'\n",
        "! git add 'Output.mp3'\n",
        "! git commit --message=\"The encoded binary data of the .mp3\"\n",
        "! git push https://HakimTem:Zero123999@github.com/HakimTem/MusicAI"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}